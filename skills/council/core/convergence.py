"""
Convergence detection for multi-round deliberation.
"""

from typing import List, Tuple
from .parsing import extract_json

# Convergence detection weights and thresholds
CONVERGENCE_CONFIDENCE_WEIGHT = 0.6  # Weight for average confidence score
CONVERGENCE_SIGNAL_WEIGHT = 0.4      # Weight for explicit convergence signals
CONVERGENCE_THRESHOLD = 0.8          # Threshold for declaring convergence


def check_convergence(round_responses: List[dict], threshold: float = CONVERGENCE_THRESHOLD) -> Tuple[bool, float]:
    """
    Check if models have converged based on:
    1. Explicit convergence signals
    2. High confidence across models

    Uses weighted combination of confidence scores and explicit signals.

    Args:
        round_responses: List of response dicts from each round
        threshold: Convergence threshold (default 0.8)

    Returns:
        Tuple of (converged: bool, convergence_score: float)
    """
    if not round_responses:
        return False, 0.0

    latest_round = round_responses[-1]

    # Check explicit convergence signals
    convergence_signals = []
    confidences = []

    for model, response in latest_round.items():
        try:
            data = extract_json(response)
            convergence_signals.append(data.get('convergence_signal', False))
            confidences.append(data.get('confidence', 0.5))
        except Exception:
            convergence_signals.append(False)
            confidences.append(0.5)

    # Calculate convergence score using configured weights
    avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
    explicit_convergence = sum(convergence_signals) / len(convergence_signals) if convergence_signals else 0.0

    convergence_score = (avg_confidence * CONVERGENCE_CONFIDENCE_WEIGHT) + (explicit_convergence * CONVERGENCE_SIGNAL_WEIGHT)

    return convergence_score >= threshold, convergence_score
